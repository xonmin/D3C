### 하둡(Hdfs)
- 정의 : 대용량 데이터를 분산 처리 
- 자바 기반의 오픈소스 프레임워크(빅데이터 저장, 처리, 분석)
- 처음에는 오픈소스 검색 엔진인 너치(Nutch)에 적용하기 위해 시작
- 동작 원리 : 분산 파일 시스템인 HDFS(Hadoop Distributed File System)에 데이터 저장 / 분산 처리 시스템인 맵리듀스를 이용해 데이터 처리
- 하둡 파일 시스템(HDFS) : 수천대의 분산된 장비에 대용량 파일을 저장할 수 있는 기능을 제공하는 분산 파일 시스템
- MapReduce는 코드가 복잡해서 짜기가 어렵다. Query로 MapReduce 대부분의 기능을 표현할 수 있다. SQL과 유사한 HiveQL으로 데이터를 조회하거나 MapReduce와 강티 처리할 수 있는 것이 Hive

---
### 하둡의 기본 구성요소
1. HDFS(Hadoop Distributed File System)
- 네임 노드(마스터 역할) : 모든 메타데이터와 데이터 노드 관리 
  - 하트비트(3 초, 데이터 노드의 동작 여부 판단)와 블록리포트(6 시간, HDFS 에 저장된 파일에 대한 최신 정보 유지) 이용 
  - 클라이언트가 네임 노드를 통해 HDFS에 저장된 파일 접근 가능
- 데이터 노드(슬레이브 역할) : 파일 저장 
  - 네임 노드에 하트비트와 블록리포트를 주기적으로 전달
- 클라이언트는 네임 노드(마스터)에 원하는 파일이 저장된 블록의 위치 확인, 해당 블록이 저장된 데이터 노드(슬레이브)에서 직접 데이터를 조회


- DB 생성 → Python에서 공공 데이터 자료 입력
- Sqoop 명령어로 DB에서 HDFS으로 데이터 가져오기
- Hive에서 HDFS의 데이터를 확인한다.
- Zeppelin으로 접속해서 노트 생성하고 분석 수행한다.

---
### 맵 리듀스(MapReduce)
- 저장된 파일 데이터를 분산된 서버의 CPU 와 메모리 자원을 이용해서 쉽고 빠르게 분석 할 수 있는 컴퓨팅 프레임워크
- 맵(Map) : 분산되어 있는 데이터를 연관성 있는 데이터들로 분류하는 작업(Key, Value 의 형태)
- 리듀스(Reduce) : Map 에서 출력된 데이터를 중복 데이터를 제거하고 원하는 데이터를 추출하는 작업
- 분산처리 역할을 하는 하둡의 중심 모듈 중에 하나이다.
- Key value 쌍을 처리하는 map 함수를 설정
- 중간 결과물 형태의 Key value 쌍 데이터를 만들고
- reduce 함수를 설정해서 같은 키를 가진 값들을 합쳐 최종 결과물을 만든다.
---
### Hive
- HDFS에 있는 파일을 읽어서 쿼리로 분석을 수행한다. 
- HiveQL을 작성하면 MapReduce로 변환되서 실행된다.
- Hive 에서 만든 각 테이블의 정보는 Hive 메타 스토어라고 불리는 특별한 DB에 저장된다.
- 데이터베이스가 아닌 데이터 처리를 위한 배치 처리 구조이다.
- 가능한 서브 쿼리에서 중간 데이터를 줄여주어야 한다. 그냥 JOIN 하면 중간 데이터가 커져서, 메모리 낭비가 생긴다.
---
### 아파치 스파크 Spark
- 실시간성 데이터에 대한 니즈로, 하둡보다 더 빠르게 사용하기 위한, 통합된 데이터 분석 엔진
- 분산 시스템을 사용한 프로그래밍 환경으로, 대량의 메모리를 활용해서 고속화
- 하둡(디스크 기반)과 같이 사용 가능하다.
- In-memory(인 메모리) 상에서 동작해서, 반복적인 처리가 필요한 작업에서 속도가 하둡보다 1000배 이상 빠르다.
- 최근 아키텍처 : 하둡의 Yarn(리소스 매니저) 위에 스파크를 얹고, 실시간이 필요한 데이터는 스파크로 처리
- 구성 요소
    1. Driver Program : main() 함수를 실행하고 SparkContext를 생성하는 프로세스
    2. Cluster Manager : 자원을 할당, 제거하는 등 클러스터 자원을 관리하는 서비스
        - Standalone, Hadoop Yarn, Apache Mesos 지원
    3. Worker Node : 클러스터에서 어플리케이션 코드를 실행하는 코드
    4. Executor : Task 들을 실행하고 메모리나 디스크에 데이터를 유지.
        - 애플리케이션의 수명 주기와 동일하게 수행된다.
